{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense,Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_test_data = pd.read_csv(\"mitbih_test.csv\", header=None)\n",
    "mit_train_data = pd.read_csv(\"mitbih_train.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21892 entries, 0 to 21891\n",
      "Columns: 188 entries, 0 to 187\n",
      "dtypes: float64(188)\n",
      "memory usage: 31.4 MB\n"
     ]
    }
   ],
   "source": [
    "mit_test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 87554 entries, 0 to 87553\n",
      "Columns: 188 entries, 0 to 187\n",
      "dtypes: float64(188)\n",
      "memory usage: 125.6 MB\n"
     ]
    }
   ],
   "source": [
    "mit_train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = mit_test_data.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleX = sample.iloc[:,sample.columns != 187]#remove target column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "Type\tCount\n",
      "0.0    72471\n",
      "4.0     6431\n",
      "2.0     5788\n",
      "1.0     2223\n",
      "3.0      641\n",
      "Name: 187, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data\")\n",
    "print(\"Type\\tCount\")\n",
    "print((mit_train_data[187]).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data\n",
      "Type\tCount\n",
      "0.0    18118\n",
      "4.0     1608\n",
      "2.0     1448\n",
      "1.0      556\n",
      "3.0      162\n",
      "Name: 187, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Test data\")\n",
    "print(\"Type\\tCount\")\n",
    "print((mit_test_data[187]).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate features and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 87554 entries, 0 to 87553\n",
      "Columns: 187 entries, 0 to 186\n",
      "dtypes: float64(187)\n",
      "memory usage: 124.9 MB\n"
     ]
    }
   ],
   "source": [
    "x= mit_train_data.loc[:, mit_train_data.columns != 187]\n",
    "x.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=mit_train_data.loc[:,mit_train_data.columns==187]\n",
    "y=to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.758264</td>\n",
       "      <td>0.111570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080579</td>\n",
       "      <td>0.078512</td>\n",
       "      <td>0.066116</td>\n",
       "      <td>0.049587</td>\n",
       "      <td>0.047521</td>\n",
       "      <td>0.035124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.908425</td>\n",
       "      <td>0.783883</td>\n",
       "      <td>0.531136</td>\n",
       "      <td>0.362637</td>\n",
       "      <td>0.366300</td>\n",
       "      <td>0.344322</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.296703</td>\n",
       "      <td>0.300366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.730088</td>\n",
       "      <td>0.212389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119469</td>\n",
       "      <td>0.101770</td>\n",
       "      <td>0.101770</td>\n",
       "      <td>0.110619</td>\n",
       "      <td>0.123894</td>\n",
       "      <td>0.115044</td>\n",
       "      <td>0.132743</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910417</td>\n",
       "      <td>0.681250</td>\n",
       "      <td>0.472917</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.068750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.014583</td>\n",
       "      <td>0.054167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.570470</td>\n",
       "      <td>0.399329</td>\n",
       "      <td>0.238255</td>\n",
       "      <td>0.147651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>0.080537</td>\n",
       "      <td>0.070470</td>\n",
       "      <td>0.090604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21887</th>\n",
       "      <td>0.928736</td>\n",
       "      <td>0.871264</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.742529</td>\n",
       "      <td>0.650575</td>\n",
       "      <td>0.535632</td>\n",
       "      <td>0.394253</td>\n",
       "      <td>0.250575</td>\n",
       "      <td>0.140230</td>\n",
       "      <td>0.102299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21888</th>\n",
       "      <td>0.802691</td>\n",
       "      <td>0.692078</td>\n",
       "      <td>0.587444</td>\n",
       "      <td>0.446936</td>\n",
       "      <td>0.318386</td>\n",
       "      <td>0.189836</td>\n",
       "      <td>0.118087</td>\n",
       "      <td>0.077728</td>\n",
       "      <td>0.112108</td>\n",
       "      <td>0.152466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21889</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967359</td>\n",
       "      <td>0.620178</td>\n",
       "      <td>0.347181</td>\n",
       "      <td>0.139466</td>\n",
       "      <td>0.089021</td>\n",
       "      <td>0.103858</td>\n",
       "      <td>0.100890</td>\n",
       "      <td>0.106825</td>\n",
       "      <td>0.100890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21890</th>\n",
       "      <td>0.984127</td>\n",
       "      <td>0.567460</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.575397</td>\n",
       "      <td>0.575397</td>\n",
       "      <td>0.488095</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21891</th>\n",
       "      <td>0.973970</td>\n",
       "      <td>0.913232</td>\n",
       "      <td>0.865510</td>\n",
       "      <td>0.823210</td>\n",
       "      <td>0.746204</td>\n",
       "      <td>0.642082</td>\n",
       "      <td>0.547722</td>\n",
       "      <td>0.426247</td>\n",
       "      <td>0.325380</td>\n",
       "      <td>0.279826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21892 rows Ã— 187 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0      1.000000  0.758264  0.111570  0.000000  0.080579  0.078512  0.066116   \n",
       "1      0.908425  0.783883  0.531136  0.362637  0.366300  0.344322  0.333333   \n",
       "2      0.730088  0.212389  0.000000  0.119469  0.101770  0.101770  0.110619   \n",
       "3      1.000000  0.910417  0.681250  0.472917  0.229167  0.068750  0.000000   \n",
       "4      0.570470  0.399329  0.238255  0.147651  0.000000  0.003356  0.040268   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "21887  0.928736  0.871264  0.804598  0.742529  0.650575  0.535632  0.394253   \n",
       "21888  0.802691  0.692078  0.587444  0.446936  0.318386  0.189836  0.118087   \n",
       "21889  1.000000  0.967359  0.620178  0.347181  0.139466  0.089021  0.103858   \n",
       "21890  0.984127  0.567460  0.607143  0.583333  0.607143  0.575397  0.575397   \n",
       "21891  0.973970  0.913232  0.865510  0.823210  0.746204  0.642082  0.547722   \n",
       "\n",
       "            7         8         9    ...  177  178  179  180  181  182  183  \\\n",
       "0      0.049587  0.047521  0.035124  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1      0.307692  0.296703  0.300366  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2      0.123894  0.115044  0.132743  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3      0.004167  0.014583  0.054167  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4      0.080537  0.070470  0.090604  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "...         ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "21887  0.250575  0.140230  0.102299  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "21888  0.077728  0.112108  0.152466  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "21889  0.100890  0.106825  0.100890  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "21890  0.488095  0.392857  0.238095  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "21891  0.426247  0.325380  0.279826  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "       184  185  186  \n",
       "0      0.0  0.0  0.0  \n",
       "1      0.0  0.0  0.0  \n",
       "2      0.0  0.0  0.0  \n",
       "3      0.0  0.0  0.0  \n",
       "4      0.0  0.0  0.0  \n",
       "...    ...  ...  ...  \n",
       "21887  0.0  0.0  0.0  \n",
       "21888  0.0  0.0  0.0  \n",
       "21889  0.0  0.0  0.0  \n",
       "21890  0.0  0.0  0.0  \n",
       "21891  0.0  0.0  0.0  \n",
       "\n",
       "[21892 rows x 187 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test=mit_test_data.loc[:,mit_test_data.columns !=187]\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=mit_test_data.loc[:,mit_test_data.columns==187]\n",
    "y_test=to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers\n",
    "model.add(Dense(50,activation='relu',input_shape=(187,)))\n",
    "model.add(Dense(50,activation=\"relu\"))\n",
    "model.add(Dense(5,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2737/2737 [==============================] - 10s 2ms/step - loss: 0.2577 - accuracy: 0.9297 - val_loss: 0.1704 - val_accuracy: 0.9541\n",
      "Epoch 2/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.1515 - accuracy: 0.9594 - val_loss: 0.1466 - val_accuracy: 0.9588\n",
      "Epoch 3/100\n",
      "2737/2737 [==============================] - 6s 2ms/step - loss: 0.1290 - accuracy: 0.9648 - val_loss: 0.1355 - val_accuracy: 0.9642\n",
      "Epoch 4/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.1158 - accuracy: 0.9686 - val_loss: 0.1397 - val_accuracy: 0.9630\n",
      "Epoch 5/100\n",
      "2737/2737 [==============================] - 4s 2ms/step - loss: 0.1061 - accuracy: 0.9704 - val_loss: 0.1124 - val_accuracy: 0.9703\n",
      "Epoch 6/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.1003 - accuracy: 0.9722 - val_loss: 0.1118 - val_accuracy: 0.9697\n",
      "Epoch 7/100\n",
      "2737/2737 [==============================] - 6s 2ms/step - loss: 0.0942 - accuracy: 0.9734 - val_loss: 0.1049 - val_accuracy: 0.9724\n",
      "Epoch 8/100\n",
      "2737/2737 [==============================] - 4s 2ms/step - loss: 0.0898 - accuracy: 0.9745 - val_loss: 0.1049 - val_accuracy: 0.9727\n",
      "Epoch 9/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0864 - accuracy: 0.9754 - val_loss: 0.1057 - val_accuracy: 0.9731\n",
      "Epoch 10/100\n",
      "2737/2737 [==============================] - 6s 2ms/step - loss: 0.0834 - accuracy: 0.9759 - val_loss: 0.0952 - val_accuracy: 0.9749\n",
      "Epoch 11/100\n",
      "2737/2737 [==============================] - 6s 2ms/step - loss: 0.0798 - accuracy: 0.9768 - val_loss: 0.0994 - val_accuracy: 0.9738\n",
      "Epoch 12/100\n",
      "2737/2737 [==============================] - 7s 2ms/step - loss: 0.0775 - accuracy: 0.9778 - val_loss: 0.0961 - val_accuracy: 0.9752\n",
      "Epoch 13/100\n",
      "2737/2737 [==============================] - 6s 2ms/step - loss: 0.0752 - accuracy: 0.9778 - val_loss: 0.1000 - val_accuracy: 0.9741\n",
      "Epoch 14/100\n",
      "2737/2737 [==============================] - 7s 2ms/step - loss: 0.0723 - accuracy: 0.9790 - val_loss: 0.0936 - val_accuracy: 0.9757\n",
      "Epoch 15/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0704 - accuracy: 0.9795 - val_loss: 0.0993 - val_accuracy: 0.9746\n",
      "Epoch 16/100\n",
      "2737/2737 [==============================] - 4s 2ms/step - loss: 0.0690 - accuracy: 0.9799 - val_loss: 0.0912 - val_accuracy: 0.9757\n",
      "Epoch 17/100\n",
      "2737/2737 [==============================] - 6s 2ms/step - loss: 0.0671 - accuracy: 0.9802 - val_loss: 0.0880 - val_accuracy: 0.9779\n",
      "Epoch 18/100\n",
      "2737/2737 [==============================] - 3s 1ms/step - loss: 0.0656 - accuracy: 0.9807 - val_loss: 0.1013 - val_accuracy: 0.9749\n",
      "Epoch 19/100\n",
      "2737/2737 [==============================] - 4s 1ms/step - loss: 0.0637 - accuracy: 0.9813 - val_loss: 0.0895 - val_accuracy: 0.9764\n",
      "Epoch 20/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0619 - accuracy: 0.9816 - val_loss: 0.0929 - val_accuracy: 0.9764\n",
      "Epoch 21/100\n",
      "2737/2737 [==============================] - 7s 3ms/step - loss: 0.0610 - accuracy: 0.9819 - val_loss: 0.0942 - val_accuracy: 0.9757\n",
      "Epoch 22/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0600 - accuracy: 0.9817 - val_loss: 0.0882 - val_accuracy: 0.9772\n",
      "Epoch 23/100\n",
      "2737/2737 [==============================] - 4s 1ms/step - loss: 0.0585 - accuracy: 0.9824 - val_loss: 0.0933 - val_accuracy: 0.9767\n",
      "Epoch 24/100\n",
      "2737/2737 [==============================] - 4s 1ms/step - loss: 0.0574 - accuracy: 0.9822 - val_loss: 0.0901 - val_accuracy: 0.9770\n",
      "Epoch 25/100\n",
      "2737/2737 [==============================] - 7s 2ms/step - loss: 0.0561 - accuracy: 0.9833 - val_loss: 0.0910 - val_accuracy: 0.9767\n",
      "Epoch 26/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0551 - accuracy: 0.9832 - val_loss: 0.1266 - val_accuracy: 0.9648\n",
      "Epoch 27/100\n",
      "2737/2737 [==============================] - 4s 1ms/step - loss: 0.0539 - accuracy: 0.9833 - val_loss: 0.0934 - val_accuracy: 0.9755\n",
      "Epoch 28/100\n",
      "2737/2737 [==============================] - 4s 1ms/step - loss: 0.0524 - accuracy: 0.9841 - val_loss: 0.0911 - val_accuracy: 0.9779\n",
      "Epoch 29/100\n",
      "2737/2737 [==============================] - 4s 1ms/step - loss: 0.0523 - accuracy: 0.9836 - val_loss: 0.0938 - val_accuracy: 0.9756\n",
      "Epoch 30/100\n",
      "2737/2737 [==============================] - 4s 1ms/step - loss: 0.0510 - accuracy: 0.9845 - val_loss: 0.0948 - val_accuracy: 0.9753\n",
      "Epoch 31/100\n",
      "2737/2737 [==============================] - 4s 1ms/step - loss: 0.0500 - accuracy: 0.9842 - val_loss: 0.0930 - val_accuracy: 0.9779\n",
      "Epoch 32/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0493 - accuracy: 0.9846 - val_loss: 0.0984 - val_accuracy: 0.9749\n",
      "Epoch 33/100\n",
      "2737/2737 [==============================] - 4s 2ms/step - loss: 0.0490 - accuracy: 0.9847 - val_loss: 0.0978 - val_accuracy: 0.9767\n",
      "Epoch 34/100\n",
      "2737/2737 [==============================] - 4s 2ms/step - loss: 0.0473 - accuracy: 0.9854 - val_loss: 0.0957 - val_accuracy: 0.9771\n",
      "Epoch 35/100\n",
      "2737/2737 [==============================] - 6s 2ms/step - loss: 0.0467 - accuracy: 0.9852 - val_loss: 0.0918 - val_accuracy: 0.9778\n",
      "Epoch 36/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0467 - accuracy: 0.9855 - val_loss: 0.0997 - val_accuracy: 0.9774\n",
      "Epoch 37/100\n",
      "2737/2737 [==============================] - 6s 2ms/step - loss: 0.0456 - accuracy: 0.9858 - val_loss: 0.0944 - val_accuracy: 0.9776\n",
      "Epoch 38/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0450 - accuracy: 0.9861 - val_loss: 0.0981 - val_accuracy: 0.9779\n",
      "Epoch 39/100\n",
      "2737/2737 [==============================] - 7s 3ms/step - loss: 0.0440 - accuracy: 0.9862 - val_loss: 0.1009 - val_accuracy: 0.9777\n",
      "Epoch 40/100\n",
      "2737/2737 [==============================] - 6s 2ms/step - loss: 0.0430 - accuracy: 0.9865 - val_loss: 0.0973 - val_accuracy: 0.9773\n",
      "Epoch 41/100\n",
      "2737/2737 [==============================] - 6s 2ms/step - loss: 0.0421 - accuracy: 0.9866 - val_loss: 0.1029 - val_accuracy: 0.9775\n",
      "Epoch 42/100\n",
      "2737/2737 [==============================] - 4s 1ms/step - loss: 0.0427 - accuracy: 0.9865 - val_loss: 0.1005 - val_accuracy: 0.9756\n",
      "Epoch 43/100\n",
      "2737/2737 [==============================] - 6s 2ms/step - loss: 0.0413 - accuracy: 0.9867 - val_loss: 0.1005 - val_accuracy: 0.9768\n",
      "Epoch 44/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0411 - accuracy: 0.9868 - val_loss: 0.0973 - val_accuracy: 0.9787\n",
      "Epoch 45/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0410 - accuracy: 0.9871 - val_loss: 0.0956 - val_accuracy: 0.9787\n",
      "Epoch 46/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0403 - accuracy: 0.9874 - val_loss: 0.0997 - val_accuracy: 0.9773\n",
      "Epoch 47/100\n",
      "2737/2737 [==============================] - 4s 2ms/step - loss: 0.0391 - accuracy: 0.9872 - val_loss: 0.1035 - val_accuracy: 0.9782\n",
      "Epoch 48/100\n",
      "2737/2737 [==============================] - 4s 2ms/step - loss: 0.0390 - accuracy: 0.9875 - val_loss: 0.0977 - val_accuracy: 0.9777\n",
      "Epoch 49/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0400 - accuracy: 0.9873 - val_loss: 0.1002 - val_accuracy: 0.9780\n",
      "Epoch 50/100\n",
      "2737/2737 [==============================] - 6s 2ms/step - loss: 0.0382 - accuracy: 0.9873 - val_loss: 0.1128 - val_accuracy: 0.9729\n",
      "Epoch 51/100\n",
      "2737/2737 [==============================] - 4s 2ms/step - loss: 0.0376 - accuracy: 0.9877 - val_loss: 0.1039 - val_accuracy: 0.9757\n",
      "Epoch 52/100\n",
      "2737/2737 [==============================] - 6s 2ms/step - loss: 0.0377 - accuracy: 0.9876 - val_loss: 0.1048 - val_accuracy: 0.9777\n",
      "Epoch 53/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0371 - accuracy: 0.9879 - val_loss: 0.1032 - val_accuracy: 0.9758\n",
      "Epoch 54/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0369 - accuracy: 0.9879 - val_loss: 0.1044 - val_accuracy: 0.9781\n",
      "Epoch 55/100\n",
      "2737/2737 [==============================] - 4s 2ms/step - loss: 0.0356 - accuracy: 0.9881 - val_loss: 0.1041 - val_accuracy: 0.9783\n",
      "Epoch 56/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0362 - accuracy: 0.9881 - val_loss: 0.1007 - val_accuracy: 0.9765\n",
      "Epoch 57/100\n",
      "2737/2737 [==============================] - 4s 1ms/step - loss: 0.0357 - accuracy: 0.9883 - val_loss: 0.1096 - val_accuracy: 0.9758\n",
      "Epoch 58/100\n",
      "2737/2737 [==============================] - 4s 1ms/step - loss: 0.0349 - accuracy: 0.9885 - val_loss: 0.1008 - val_accuracy: 0.9762\n",
      "Epoch 59/100\n",
      "2737/2737 [==============================] - 4s 1ms/step - loss: 0.0349 - accuracy: 0.9886 - val_loss: 0.1039 - val_accuracy: 0.9778\n",
      "Epoch 60/100\n",
      "2737/2737 [==============================] - 4s 1ms/step - loss: 0.0347 - accuracy: 0.9885 - val_loss: 0.1043 - val_accuracy: 0.9771\n",
      "Epoch 61/100\n",
      "2737/2737 [==============================] - 4s 1ms/step - loss: 0.0343 - accuracy: 0.9888 - val_loss: 0.1083 - val_accuracy: 0.9783\n",
      "Epoch 62/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0341 - accuracy: 0.9887 - val_loss: 0.1080 - val_accuracy: 0.9791\n",
      "Epoch 63/100\n",
      "2737/2737 [==============================] - 4s 2ms/step - loss: 0.0335 - accuracy: 0.9889 - val_loss: 0.1056 - val_accuracy: 0.9767\n",
      "Epoch 64/100\n",
      "2737/2737 [==============================] - 4s 2ms/step - loss: 0.0331 - accuracy: 0.9892 - val_loss: 0.1100 - val_accuracy: 0.9778\n",
      "Epoch 65/100\n",
      "2737/2737 [==============================] - 4s 2ms/step - loss: 0.0332 - accuracy: 0.9890 - val_loss: 0.1098 - val_accuracy: 0.9773\n",
      "Epoch 66/100\n",
      "2737/2737 [==============================] - 4s 1ms/step - loss: 0.0315 - accuracy: 0.9895 - val_loss: 0.1142 - val_accuracy: 0.9789\n",
      "Epoch 67/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0319 - accuracy: 0.9896 - val_loss: 0.1123 - val_accuracy: 0.9767\n",
      "Epoch 68/100\n",
      "2737/2737 [==============================] - 4s 2ms/step - loss: 0.0316 - accuracy: 0.9893 - val_loss: 0.1167 - val_accuracy: 0.9767\n",
      "Epoch 69/100\n",
      "2737/2737 [==============================] - 4s 2ms/step - loss: 0.0319 - accuracy: 0.9895 - val_loss: 0.1126 - val_accuracy: 0.9763\n",
      "Epoch 70/100\n",
      "2737/2737 [==============================] - 4s 2ms/step - loss: 0.0316 - accuracy: 0.9896 - val_loss: 0.1098 - val_accuracy: 0.9773\n",
      "Epoch 71/100\n",
      "2737/2737 [==============================] - 7s 2ms/step - loss: 0.0319 - accuracy: 0.9893 - val_loss: 0.1157 - val_accuracy: 0.9781\n",
      "Epoch 72/100\n",
      "2737/2737 [==============================] - 6s 2ms/step - loss: 0.0308 - accuracy: 0.9899 - val_loss: 0.1218 - val_accuracy: 0.9766\n",
      "Epoch 73/100\n",
      "2737/2737 [==============================] - 4s 1ms/step - loss: 0.0305 - accuracy: 0.9900 - val_loss: 0.1185 - val_accuracy: 0.9776\n",
      "Epoch 74/100\n",
      "2737/2737 [==============================] - 4s 2ms/step - loss: 0.0303 - accuracy: 0.9894 - val_loss: 0.1112 - val_accuracy: 0.9785\n",
      "Epoch 75/100\n",
      "2737/2737 [==============================] - 4s 1ms/step - loss: 0.0299 - accuracy: 0.9897 - val_loss: 0.1143 - val_accuracy: 0.9784\n",
      "Epoch 76/100\n",
      "2737/2737 [==============================] - 4s 2ms/step - loss: 0.0293 - accuracy: 0.9902 - val_loss: 0.1187 - val_accuracy: 0.9769\n",
      "Epoch 77/100\n",
      "2737/2737 [==============================] - 4s 2ms/step - loss: 0.0291 - accuracy: 0.9902 - val_loss: 0.1108 - val_accuracy: 0.9789\n",
      "Epoch 78/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0296 - accuracy: 0.9900 - val_loss: 0.1157 - val_accuracy: 0.9780\n",
      "Epoch 79/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0293 - accuracy: 0.9899 - val_loss: 0.1204 - val_accuracy: 0.9760\n",
      "Epoch 80/100\n",
      "2737/2737 [==============================] - 4s 1ms/step - loss: 0.0292 - accuracy: 0.9904 - val_loss: 0.1186 - val_accuracy: 0.9783\n",
      "Epoch 81/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0277 - accuracy: 0.9909 - val_loss: 0.1265 - val_accuracy: 0.9777\n",
      "Epoch 82/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0285 - accuracy: 0.9905 - val_loss: 0.1320 - val_accuracy: 0.9773\n",
      "Epoch 83/100\n",
      "2737/2737 [==============================] - 4s 1ms/step - loss: 0.0287 - accuracy: 0.9903 - val_loss: 0.1166 - val_accuracy: 0.9774\n",
      "Epoch 84/100\n",
      "2737/2737 [==============================] - 6s 2ms/step - loss: 0.0278 - accuracy: 0.9906 - val_loss: 0.1198 - val_accuracy: 0.9774\n",
      "Epoch 85/100\n",
      "2737/2737 [==============================] - 6s 2ms/step - loss: 0.0271 - accuracy: 0.9909 - val_loss: 0.1197 - val_accuracy: 0.9773\n",
      "Epoch 86/100\n",
      "2737/2737 [==============================] - 8s 3ms/step - loss: 0.0271 - accuracy: 0.9910 - val_loss: 0.1218 - val_accuracy: 0.9771\n",
      "Epoch 87/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0278 - accuracy: 0.9904 - val_loss: 0.1232 - val_accuracy: 0.9764\n",
      "Epoch 88/100\n",
      "2737/2737 [==============================] - 4s 1ms/step - loss: 0.0272 - accuracy: 0.9908 - val_loss: 0.1186 - val_accuracy: 0.9773\n",
      "Epoch 89/100\n",
      "2737/2737 [==============================] - 6s 2ms/step - loss: 0.0263 - accuracy: 0.9910 - val_loss: 0.1219 - val_accuracy: 0.9760\n",
      "Epoch 90/100\n",
      "2737/2737 [==============================] - 4s 2ms/step - loss: 0.0275 - accuracy: 0.9911 - val_loss: 0.1232 - val_accuracy: 0.9779\n",
      "Epoch 91/100\n",
      "2737/2737 [==============================] - 6s 2ms/step - loss: 0.0255 - accuracy: 0.9913 - val_loss: 0.1296 - val_accuracy: 0.9773\n",
      "Epoch 92/100\n",
      "2737/2737 [==============================] - 4s 1ms/step - loss: 0.0273 - accuracy: 0.9911 - val_loss: 0.1243 - val_accuracy: 0.9781\n",
      "Epoch 93/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0250 - accuracy: 0.9915 - val_loss: 0.1349 - val_accuracy: 0.9783\n",
      "Epoch 94/100\n",
      "2737/2737 [==============================] - 6s 2ms/step - loss: 0.0260 - accuracy: 0.9913 - val_loss: 0.1193 - val_accuracy: 0.9779\n",
      "Epoch 95/100\n",
      "2737/2737 [==============================] - 7s 2ms/step - loss: 0.0254 - accuracy: 0.9913 - val_loss: 0.1301 - val_accuracy: 0.9773\n",
      "Epoch 96/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0247 - accuracy: 0.9914 - val_loss: 0.1342 - val_accuracy: 0.9767\n",
      "Epoch 97/100\n",
      "2737/2737 [==============================] - 7s 2ms/step - loss: 0.0248 - accuracy: 0.9916 - val_loss: 0.1417 - val_accuracy: 0.9767\n",
      "Epoch 98/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0254 - accuracy: 0.9915 - val_loss: 0.1211 - val_accuracy: 0.9763\n",
      "Epoch 99/100\n",
      "2737/2737 [==============================] - 5s 2ms/step - loss: 0.0242 - accuracy: 0.9917 - val_loss: 0.1424 - val_accuracy: 0.9771\n",
      "Epoch 100/100\n",
      "2737/2737 [==============================] - 4s 2ms/step - loss: 0.0238 - accuracy: 0.9920 - val_loss: 0.1314 - val_accuracy: 0.9765\n"
     ]
    }
   ],
   "source": [
    "train_ep=model.fit(x,y,validation_data=(x_test,y_test),epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: \n",
      "685/685 [==============================] - 1s 1ms/step - loss: 0.1314 - accuracy: 0.9765\n",
      "Mean Square Error: 0.13142289221286774\n",
      "Accuracy: 0.9765210747718811\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation: \")\n",
    "mse, acc=model.evaluate(x_test,y_test)\n",
    "print(\"Mean Square Error:\",mse)\n",
    "print(\"Accuracy:\",acc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "707aab0a078b6b4ae7605615109a6f7159987081daedafbf3334295fb1ab1b6c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
